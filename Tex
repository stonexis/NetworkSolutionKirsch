\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
%\usepackage[cp1251]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[english , russian]{babel}
\usepackage{a4wide}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amsbsy}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{accents}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,calc}
\graphicspath{ {./imag} } 
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usepackage{bm}

\tikzset{myarr/.style={-{Stealth[length=3pt,width=4pt]},line width=0.6pt}}

% 1) square + single DOWN arrow + large "c"
\newcommand{\symSq}{%
  \tikz[baseline=-0.8ex,scale=1]{
    \draw[line width=0.6pt] (-0.6,-0.6) rectangle (0.6,0.6);
    \node at (0,0) {\Large $c$};
    \draw[myarr] (0,-0.6) -- (0,-1.2);
  }%
}

\newcommand{\symB}{%
  \tikz[baseline=-0.8ex,scale=1]{
    \draw[line width=0.6pt] (-0.6,-0.6) rectangle (0.6,0.6);
    \node at (0,0) {\Large $b$};
    \draw[myarr] (0,-0.6) -- (0,-1.2);
  }%
}

% 2) circle + TWO DOWN arrows + large "γ×"
\newcommand{\symGam}{%
  \tikz[baseline=-0.8ex,scale=1]{
    \draw[line width=0.6pt] (0,0) circle (0.65);
    \node at (0,0) {\Large $\times\gamma$};
    % two downward arrows: one starting above, one at bottom
    \draw[myarr] (0,1.15) -- (0,0.65);
    \draw[myarr] (0,-0.65) -- (0,-1.25);
  }%
}

\newcommand{\symW}{%
  \tikz[baseline=-0.8ex,scale=1]{
    \draw[line width=0.6pt] (0,0) circle (0.65);
    \node at (0,0) {\Large $\times w$};
    % two downward arrows: one starting above, one at bottom
    \draw[myarr] (0,1.15) -- (0,0.65);
    \draw[myarr] (0,-0.65) -- (0,-1.25);
  }%
}

% 3) circle + TWO DOWN arrows + large "θ"
\newcommand{\symThe}{%
  \tikz[baseline=-0.8ex,scale=1]{
    \draw[line width=0.6pt] (0,0) circle (0.65);
    \node at (0,0) {\Large $\Theta$};
    \draw[myarr] (0,1.15) -- (0,0.65);
    \draw[myarr] (0,-0.65) -- (0,-1.25);
  }%
}

\newcommand{\symSgn}{%
  \tikz[baseline=-0.8ex,scale=1]{
    \draw[line width=0.6pt] (0,0) circle (0.65);
    \node at (0,0) {\large $sgn()$};
    \draw[myarr] (0,1.15) -- (0,0.65);
    \draw[myarr] (0,-0.65) -- (0,-1.25);
  }%
}

% 4) upside‐down triangle + THREE DOWN arrows + large "+"
\newcommand{\symTri}{%
  \tikz[baseline=-0.8ex,scale=1]{
    \coordinate (A) at (-0.75,0.65);
    \coordinate (B) at (0.75,0.65);
    \coordinate (C) at (0,-0.85);
    \draw[line width=0.6pt] (A) -- (B) -- (C) -- cycle;
    \node at (0,0.0) {\Large $\Sigma$};
    % two inputs: arrows down into A & B
    \draw[myarr] ($(A)+(0,0.7)$) -- (A);
    \draw[myarr] ($(B)+(0,0.7)$) -- (B);
    % one output: arrow down from C
    \draw[myarr] (C) -- ($(C)+(0,-0.7)$);
  }%
}


\begin{document}


 
\begin{titlepage}
 

 \begin{center}
 \large
 
 МОСКОВСКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ \\ ИМЕНИ М. В. ЛОМОНОСОВА
 \vspace{0.25cm}
 
 Механико-математический факультет
 
 Кафедра механики композитов
 \vfill
 
 
 Шелепов Павел Игоревич
 \vfill
 
 \textsc{Курсовая работа}\\[5mm]
 
 {\Large {«Построение нейросетевой аппроксимации решения задачи Кирша» \bigskip }
 
 (Development of a neural-network-based approximation for Kirsch’s problem) 
 }
 \bigskip
 
 4 курс, группа 425
 \end{center}
 \vfill
 
 \newlength{\ML}
 \settowidth{\ML}{«\underline{\hspace{0.7cm}}» \underline{\hspace{2cm}}}
 \hfill\begin{minipage}{0.4\textwidth}
 Руководитель курсовой работы\\
 \underline{\hspace{2cm}} н.с. А.\,В.~Романов\\
 «\underline{\hspace{0.7cm}}» \underline{\hspace{2cm}} 2025 г.
 \end{minipage}%
 \bigskip
 
 
 \begin{center}
 Москва, 2025 г.
 \end{center}
\end{titlepage}
\newpage
\tableofcontents
\newpage
\begin{center}
\section*{Введение. Цели и задачи работы.}
\addcontentsline{toc}{section}{Введение.}
\end{center}

Решение уравнений теории упругости для конструкционных элементов с локальными стресс-концентраторами традиционно выполняют либо аналитически (для упрощённых моделей) либо численно, методом конечных элементов. Однако аналитические решения, подобно классическому результату Кирша для круглого отверстия, ограничены простыми геометриями, а детальный анализ методом конечных элементов каждой новой конфигурации требует значительного вычислительного времени, что становится критичным при оптимизационных и мультизапусковых задачах современной инженерии.\bigskip

Параллельно с развитием классических подходов в последнее десятилетие активно исследуются методы глубокого обучения, доказавшие высокую эффективность в распознавании образов и решении обратных задач физики. Свёрточные нейронные сети, обладающие свойством трансляционной эквивариантности, естественно применимы к полевым данным, представленным на регулярной сетке. Использование архитектуры энкодер–декодер позволяет одновременно захватывать глобальный контекст граничных условий и восстанавливать мелкомасштабные детали напряжённого состояния.\bigskip

Настоящая работа посвящена построению аппроксимации методами глубокого обучения решения задачи Кирша — определения поля напряжений около круглого отверстия в растягиваемой пластине. В качестве эталона применяется аналитическое выражение, что исключает влияние ошибок «численного эталона» и позволяет сформировать обучающую выборку произвольного объёма. Основная цель исследования — показать, что правильно спроектированная  сеть способна воспроизводить трёхкомпонентное поле напряжений с малой относительной погрешностью при времени расчета порядка миллисекунд, тем самым открывая путь к мгновенной оценке концентраций напряжений в параметрических и оптимизационных расчётах.

\newpage
 \begin{center}
\item  \section{Основные положения. Принципы работы нейронных сетей}
 \item  \subsection{Нейронная сеть как схема из функциональных элементов (СФЭ)}
\end{center}

Рассмотрим нейронную схему модели МакКаллока--Питтса, построенную над базисом структурных элементов
$${\Large \bm{B}} = \Bigg\{\,\symSq,\;\symGam,\;\symThe,\;\symTri\Bigg\} \eqno(1)$$
где

\[
\Theta(x)=
\begin{cases}
1,&x\ge 0,\\
0,&x<0
\end{cases}
\quad\text{― функция Хевисайда},
\tag{2}
\]
$\Theta(x)=1$ трактуется как ``нейрон активирован'', $\Theta(x)=0$~― ``не активирован''; \\
$c\in\mathbb R$ ― генератор постоянного сигнала (порог), добавляющий смещение к сумме входов; \\
$\gamma\in\mathbb R$ ― весовой множитель, масштабирующий вход; \\
$\Sigma$ ― сумматор, вычисляющий линейную комбинацию $n$~входов, $n\in\mathbb N\setminus\{1\}$.

Полное уравнение порогового нейрона:
\[
y^{\text{out}}=\Theta\!\Bigl(\sum_{i=1}^{n}\gamma_i x^{\text{in}}_i+c\Bigr).
\tag{3}
\]

Запишем скалярное произведение компактно:  
$\sum_{i=1}^{n}\gamma_i x_i = \langle\boldsymbol\gamma,\mathbf x\rangle$.  
$$ \text{Тогда \; }\Theta(\langle\boldsymbol\gamma,\mathbf x\rangle+c)=1 \text{\; описывает полупространство:}$$ 
$$
H^+=\Bigl\{\mathbf x\in\mathbb R^{n}\mid\langle\boldsymbol\gamma,\mathbf x\rangle+c\ge 0\Bigr\}.
\eqno(4)$$
Следовательно, один пороговый нейрон реализует индикатор полупространства, а значит, способен корректно классифицировать \emph{только линейно разделимые} множества.  
Например, для булевого случая $n=2$ невозможно представить функцию XOR:

$$
\begin{matrix}
(0,0)\mapsto 0,\;(0,1)\mapsto 1,\;(1,0)\mapsto 1,\;(1,1)\mapsto 0
\end{matrix}
$$
$$
\not\exists\,(\boldsymbol\gamma,c):\;
\Theta(\langle\boldsymbol\gamma,\mathbf x\rangle+c)=\text{XOR}(\mathbf x).
$$

Подчеркнём, что ограничение обусловлено \emph{линейностью ядра} $\langle\boldsymbol\gamma,\mathbf x\rangle$ и двоичной активацией~$\Theta$.
Развитием данного подхода стал Перцептрон Розенблатта - \emph{обучаемый} пороговый классификатор. Его базис из функциональных элементов:
$${\Large \bm{B'}} = \Bigg\{\,\symB,\;\symW,\;\symSgn,\;\symTri\Bigg\} \eqno(5)$$

Где $\text{sgn}(x)=\Theta(x)-\Theta(-x)$ ― симметричная пороговая функция. Математически модель полностью описывается двумя уравнениями:

$$y^{out} = sgn(\sum\limits_{i=1}^{n}w_ix^{in}_i + b) \eqno(6)$$
$$\Delta w_i = \eta (d - y^{out})x_i^{in} \eqno(7)$$

Где (6) - задает закон зависимости выхода от входа, (7) - задает правило изменения параметров сети, называемое "обучением". $\eta > 0$  - скорость обучения, $d \in \{-1, 1\}$ - корректная (заранее известная) метка предсказываемого класса.

 \textbf{Теорема Новикова о сходимости перцептрона} гарантирует, что правило (6),(7) найдёт такую пару $(\mathbf w,b)$, если обучающая выборка \emph{линейно разделима}. В противном случае веса обновляются бесконечно, а ошибка не обнуляется ― практическое подтверждение упомянутого ограничения.

Чтобы преодолеть линейность, элементы~(6) или~(3) можно \emph{скомпоновать}.  
Пусть выходы $m$ пороговых нейронов первого слоя образуют вектор
\[
\mathbf z^{(1)}=\Theta\!\bigl(W^{(1)}\mathbf x+\mathbf b^{(1)}\bigr)\in\{0,1\}^{m},\eqno(8)
\]
тогда второй слой получает на вход выходы первого
\[
y^{\text{out}}=\Theta\!\bigl(W^{(2)}\mathbf z^{(1)}+b^{(2)}\bigr).\eqno(9)
\]
Двухслойная сеть уже способна \emph{реализовать любую булеву функцию}.  
Простейший пример ― XOR, реализуемый как:
\[
\text{XOR}(x_1,x_2)=\Theta\!\bigl(z_1-z_2-0.5\bigr),\quad
z_1=\Theta(x_1 + x_2 -0.5),\;z_2=\Theta( x_1 + x_2 - 1.5).
\]

Для построения \emph{непрерывных} функций требуются недвоичные или мягкие выходы. Если заменить последний пороговый узел линейным,
$$
y = \sum_{j=1}^{m}\beta_j\,z^{(1)}_j \text{ - получаем \emph{кусочно-постоянную аппроксимацию функций}.} $$

Именно здесь возникает связь~с \textbf{Теоремой Колмогорова–Арнольда:}
каждая непрерывная функция $f:\mathbb R^{n}\!\to\!\mathbb R$ представима в виде
\[
f(\mathbf x)=\sum_{q=0}^{2n}\Phi_q\!\Bigl(\sum_{p=1}^{n}\varphi_{q,p}(x_p)\Bigr),
\eqno(10)
\]
то есть \emph{суперпозиции непрерывных функций $ \varphi: \mathbb R\!\to\!\mathbb R$}. Пороговые нейроны играют роль
$\varphi_{q,p}$, линейная комбинация~― роль~$\Phi_q$.

Теорема отвечает на вопрос о возможности построения аппроксимации, при конечном, но неизвестном количестве функций (нейронов). Таким образом, при построении вычислительного алгоритма необходимо знать заранее количество нейронов необходимое для приближения функции (что практически невозможно) или сконструировать гибкую (многослойную) архитектуру, нивелирующую отсутствие информации о необходимом количестве нейронов. 

Таким образом, немедленно обнаруживается серьезный недостаток подхода (6)~(7), который напрямую следует из формулы~(7) - невозможно обновлять параметры сети, в которой более чем два слоя. В последующем параграфе будет показано решение этой проблемы.

 \begin{center}
 \item  \subsection{Многослойные нейронные сети}
\end{center}

\paragraph{1. Архитектура многослойного перцептрона.}

$$
\mathbf z^{(0)} = \mathbf x \in\mathbb R^{n}\eqno(11)
$$
$$
\mathbf S^{(\ell)} = W^{(\ell)}\mathbf z^{(\ell-1)}+\mathbf b^{(\ell)}, \ \ell=1,\dots,L-1; \eqno(12)
$$
$$
\mathbf z^{(\ell)} = \sigma\bigl(\mathbf S^{(\ell)}\bigr),\eqno(13)
$$
$$
\bm{y} = F(\mathbf x;\Theta)=W^{(L)}\mathbf z^{(L-1)}+\mathbf b^{(L)}.\eqno(14) 
$$


\noindent
Здесь:

(11) — Вход сети; (12) — Линейное преобразование входа для каждого скрытого слоя; называемое \emph{предактивацией}.
(13) — Применение к выходу $\bm{S}$ каждого слоя нелинейного оператора, называемого \emph{функцией активации};
$\Theta=\{W^{(\ell)},\mathbf b^{(\ell)}\}_{\ell=1}^{L}$ — совокупность параметров.  
  
Каждый слой реализует аффинное преобразование, за которым следует нелинейная активация, необходимая для создания нескольких слоев, поскольку:
\[
\forall f\in\mathrm{Lin}(V,W),\;\forall g\in\mathrm{Lin}(W,U):\;g\circ f\in\mathrm{Lin}(V,U).
\]
Итоговая сеть — композиция операторов, способная
аппроксимировать любую непрерывную функцию на компакте при достаточной ширине скрытых слоёв, как следствие теоремы (10).
Остаётся сформулировать численный критерий, по которому можно будет \emph{автоматически} подбирать параметры $\Theta$. Такой критерий вводится в виде \emph{функции потерь}, измеряющей расхождение между выходом сети и желаемым откликом на обучающей выборке. Среднее значение потерь по выборке образует \emph{эмпирический риск}, служащий целевой функцией для процедуры оптимизации.

\vspace{4pt}

\paragraph{2. Функция потерь, эмпирический риск и критерий обучения модели.}

Пусть $X \subset \mathbb R^n$  — пространство входов сети, $Y \subset \mathbb R^m$ — пространство выходов сети,  
$$F_\Theta\!:X\to Y \text{ —  параметризованное семейство моделей, }
\Theta\in\mathbb R^p \eqno(15)$$
Также пусть $\exists T$ - некоторая выборка, называемая \emph{обучаемой:}
$$ T=\{(x^{(k)},d^{(k)})\}_{k=1}^{N} \;, \; (x^{(k)},d^{(k)}) \sim \mathcal D \text{ - неизвестное распределение на}\;X\times Y$$
Введем функционал:

\emph{Истинный риск} — $R(\Theta): \mathbb R^p \to \mathbb R$ 

\[
R(\Theta)
=\; \mathbb{E}_{(x,d)\sim\mathcal D}\!\bigl[\mathcal L\bigl(F_{\Theta}(x),d\bigr)\bigr], \eqno(16)
\]
где $ \mathcal L$ - функция потерь: 
$$
\mathcal L:Y\times Y\;\longrightarrow\;[0,\infty),
\quad
\mathcal L(y,d)=0\;\Longleftrightarrow\;y=d \eqno(17)
$$
Заметим, что минимизация функционала (16) - решение задачи обучения сети, однако $R(\Theta)$ не может быть вычислен, поскольку распределение $\mathcal D$ - неизвестно для алгоритма, тем не менее возможно вычислить его аппроксимацию:


\emph{Эмпирический риск} —
функционал: $R_N(\Theta): \mathbb R^p \to \mathbb R$

\[
R_{N}(\Theta)
=\;\frac{1}{N}\sum_{k=1}^{N}
\mathcal L\bigl(F_{\Theta}(x^{(k)}),\,d^{(k)}\bigr).
\eqno(18)
\]


Предположим, что элементы тренировочной выборки  $(x^{(k)},d^{(k)}) \sim \mathcal D$ независимые и одинаково распределённые. Тогда усиленному закону больших чисел:   
$$R_{N}(\Theta)\xrightarrow[N\to\infty]{\text{п.н.}}R(\Theta), \eqno(19)$$
то есть эмпирический риск \(R_N\) является состоятельной оценкой истинного риска \(R\).

\paragraph{Принцип минимизации эмпирического риска.}

\bigskip

Искомые оптимальные параметры $\Theta^{*}$ находятся как решение задачи оптимизации:
\[
\Theta^{*}
=\underset{\Theta\in\mathbb{R}^{p}}{\arg\min}\;R_{N}(\Theta)
\;\approx\;\underset{\Theta}{\arg\min}\;R(\Theta). \eqno(20)
\]

Если $\mathcal L$ дифференцируема по первому аргументу, а $F_\Theta$
дифференцируем по $\Theta$, то 
$$
\nabla_\Theta R_N(\Theta)
=
\frac1N\sum_{k=1}^{N}\bigg([\mathcal J_{F_\Theta}(x^{(k)})]^T
\nabla_y
\mathcal L\bigl(F_\Theta(x^{(k)}),d^{(k)}\bigr)\bigg), \eqno(21)
$$
Здесь
\begin{align*}
&\mathcal J_{F_\Theta}(x) \in \mathbb R^{m\times p} \;  \text{— Якобиан модели по параметрам};\nonumber\\
&\nabla_y \mathcal L \; \text{ — Градиент функции потерь по выходу};
\end{align*}
Из выражения (21) следует, что задача (20) может быть решена градиентными методами оптимизации.

Наивное правило обновления параметров перцептрона~(6) корректирует только последний слой, поскольку только туда поступает ошибочный сигнал $d-y$. Для обновления всех слоев, независимо от расположения, необходим другой алгоритм. Из формулы (21) можно заметить, что алгоритм должен учитывать градиент по каждому параметру из набора $\Theta$.

\paragraph{3. Градиентное обучение, метод обратного распространения ошибки.} Из предыдущего пункта следует, что необходимо перенести информацию об ошибке сквозь всю
композицию (11)-(14) или математически:

$$\forall \ell: \ell<L-1, \text{ необходимо вычислить }
\frac{\partial R}{\partial W^{(\ell)}} \text{ и } \frac{\partial R}{\partial \mathbf b^{(\ell)}}.$$
Это позволяет сделать \textbf{Алгоритм обратного распространения ошибки:} 

\begin{equation}\tag{22}
\begin{aligned}
  \delta^{(L)} &= \nabla_y\mathcal L, \\[6pt]
  \delta^{(\ell)}
    &= \bigl(\mathcal J_{\sigma}(S^{\ell})\,W^{(\ell+1)T}\bigr)\,\delta^{(\ell+1)},
      \quad \ell = L-1,\dots,1, \\[6pt]
  \frac{\partial R}{\partial W^{(\ell)}} 
    &= \frac1N \sum_{k=1}^N \delta^{(\ell)}\,(\mathbf z^{(\ell-1)})^{\!\top},
    \quad
  \frac{\partial R}{\partial \mathbf b^{(\ell)}} 
    = \frac1N \sum_{k=1}^N \delta^{(\ell)}.
\end{aligned}
\end{equation}


\noindent
Процесс вычисления по формулам (22) выглядит следующим образом:

Сначала вычисляется градиент функции потерь по её входу, после чего, для всех слоев, начиная с последнего, $\ell = L-1,\dots,1$, вычисляется вектор 
$\delta^{(\ell)}$, называемый «\emph{ошибкой}» или «\emph{чувствительностью}», который в конечном итоге, позволяет вычислить градиент функции риска $R$ по всем параметрам сети, путём усреднения по выборке $T$ значений, полученных на предыдущих шагах алгоритма.

Обновление параметров $W,\bm{b}$ обычно осуществляется градиентными методами, для простоты выберем градиентный спуск:
\[
W^{(\ell)}\leftarrow W^{(\ell)}-\eta\,\frac{\partial R}{\partial W^{(\ell)}},\qquad
\mathbf b^{(\ell)}\leftarrow\mathbf b^{(\ell)}-\eta\,\frac{\partial R}{\partial\mathbf b^{(\ell)}}.
\tag{23}
\]

\vspace{2pt}
\textbf{Ключевой результат:}
обратное распространение ошибки делает возможным совместное обучение
всех слоёв независимо от их удалённости от выхода.  
Тем самым устранены ограничения схем (5) – (7):  
\emph{глубина сети произвольна, параметры определяются автоматически,
а выбор функции потерь позволяет адаптировать сеть к
конкретной постановке задачи (классификация, регрессия, сегментация и т. п.).}

 \begin{center}
\item  \section{Современные глубокие сети. Построение нейросетевой аппроксимации}
 \item  \subsection{Сверточные нейронные сети}
\end{center}
 
В рассматриваемой работе механическая задача Кирша (подробно будет изложена в следующем разделе) естественно формулируется в терминах трёхканальных карт полей напряжений на прямоугольной сетке. Такое представление эквивалентно изображению, поэтому для его обработки рационально применить архитектуры, разработанные именно для двумерных данных — свёрточные нейронные сети, а не классические полносвязные, описанные выше.

Архитектура свёрточной нейронной сети опирается на два ключевых принципа:

Локальная связность — каждый выходной элемент зависит только от небольшого окна входа;

Разделяемость весов — одно и то же ядро свёртки последовательно применяется ко всему изображению, что резко снижает число параметров и делает сеть инвариантной к сдвигу признаков.

Подробнее:

\emph{Сверточный слой} определяется линейным оператором:

$$Y^k_{ij} = \sum_{c=0}^{C_{in} - 1} \sum_{m=0}^{K_{h} - 1} \sum_{n=0}^{K_{w} - 1} X^c_{i+m,j+n}K^{ck}_{mn} + b^k, \quad \forall k = 0, ..., C_{out} - 1 \eqno(24)$$

Где 

$$X \in \mathbb R^{C_{in}\times H_{in} \times W_{in}}  \text{ — входной тензор;}\eqno(25)$$

$$K \in \mathbb R^{C_{out}\times C_{in} \times K_h \times K_w} \text{ — набор фильтров, называемый \emph{ядром} свёртки};\eqno(26)$$

$$Y \in \mathbb R^{C_{out}\times (H_{in} - K_h + 1) \times (W_{in} - K_w + 1)} \text{ — результат свертки;}\eqno(27)$$

$C_{in}/C_{out}$ — количество входных/выходных карт (каналов); $K_h, K_w$ — пространственные размерности ядра свёртки; 

Далее результат этой линейной операции, как и в полносвязной сети, пропускается через нелинейность, образуя сверточный блок, который заменяет/дополняет блоки (12) - (13).

Для дальнейшего повествования понадобится также операция, обратная к свёртке:

\emph{Транспонированная свертка} - линейный оператор:

$$Z^c_{uv} = \sum_{k=0}^{C_{out} - 1} \sum_{m=0}^{K_{h} - 1} \sum_{n=0}^{K_{w} - 1} Y^k_{u-m,v-n}K^{ck}_{mn} + b^k, \quad \forall c = 0, ..., C_{in} - 1; \eqno(28)$$

$$X \in \mathbb R^{C_{out}\times H_{in} \times W_{in}}  \text{ — входной тензор;}\eqno(29)$$

$$K \in \mathbb R^{C_{in} \times C_{out} \times K_h \times K_w} \text{ — ядро транспонированной свертки;}\eqno(30)$$

$$Z \in \mathbb R^{C_{in}\times (H_{in} + K_h - 1) \times (W_{in} + K_w - 1)} \text{ — результат транспонированной свертки;}\eqno(31)$$
При индексах $u - m < 0$ или $v - n < 0$ величина $Y^k_{u-m,v-n}$ принимается равной 0.

Заметим, что результат прямой свертки имеет меньшую пространственную размерность, чем вход; в случае транспонированной свертки ситуация обратная.

 \begin{center}
 \item  \subsection{Архитектура энкодер-декодер}
\end{center}

Пусть $X=\mathbb R^{C_{\mathrm{in}}\times H\times W}$ —
пространство входных тензоров,  
$Y=\mathbb R^{C_{\mathrm{out}}\times H\times W}$ —
пространство выходных тензоров.  
Архитектура \emph{энкодер–декодер} реализует отображение  
\[
F_\Theta :\; X \longrightarrow Y,\qquad \Theta\in\mathbb R^{p}
\text{— пространство параметров модели;}\]
как композицию двух последовательностей операторов

\[
F_\Theta
=\underbrace{D_{L}\circ\cdots\circ D_{1}}_{\text{декодер}}
\;\circ\;
\underbrace{E_{1}\circ\cdots\circ E_{L}}_{\text{энкодер}},
\tag{32}
\]

где для каждого слоя $\ell=1,\dots,L$  
\[
E_{\ell}(z)=\sigma\!\bigl(A^{(\ell)}z\bigr),
\qquad
D_{\ell}(z)=\sigma\!\bigl(B^{(\ell)}z\bigr),
\eqno(33)\]
$A^{(\ell)}$ — оператор прямой свертки; $B^{(\ell)}$ — оператор транспонированной свертки;
$\sigma$ — фиксированная нелинейная активация
(непрерывная и почти всюду дифференцируемая).  
Операторы $\{E_\ell\}$ последовательно уменьшают пространственные
размеры тензора, увеличивая число каналов,
образуя \emph{сжатое} представление
$h=E_{L}\circ\cdots\circ E_{1}(x)\in\mathbb R^{C_{L}\times H_{L}\times W_{L}}$,
а операторы $\{D_\ell\}$ восстанавливают исходную
размерность, уменьшив число каналов до $C_{\mathrm{out}}$.
Таким образом композиция (32)
обеспечивает одновременный учёт
глобального контекста (благодаря уменьшению $H,W$ в энкодере)
и восстановление локальной детализации (за счёт обратного
преобразования в декодере), оставаясь
дифференцируемой по $\Theta$, что необходимо
для оптимизации методом обратного распространения ошибки. Экспериментальное доказательство возможности аппроксимации такого вида будет представлено в последующем параграфе.


 \begin{center}
\item  \section{Анализ нейросетевой аппроксимации. Сравнение с аналитическим решением}
 \item  \subsection{Постановка задачи Кирша}
\end{center}

Дана бесконечная тонкая симметричная пластинка с круговым отверстием радиуса $a$, нагружаемая симметрично, относительно своей срединной прямой нагрузкой P.
Введем систему координат $Oxy$ так, чтобы координатная ось $Ox$ совпадала с срединной прямой пластинки, а ось $Oy$ была перпендикулярна ей.
$$\text{Будем считать, что вектор перемещения } \; \overline{u} = (u_1,u_2) \text{ имеет следующие компоненты: }$$ $$ u_1 = u_1(x,y); \; u_2 = u_2(x,y)$$
\\Компоненты тензора деформации при этом примут вид:
$$\varepsilon_{11} = \frac{\partial u_1}{\partial x};\; \varepsilon_{22} = \frac{\partial u_2}{\partial y};\;
\varepsilon_{12} = \frac{1}{2}\bigg(\frac{\partial u_1}{\partial y} +\frac{\partial u_2}{\partial x}\bigg)\eqno(34)$$
Компоненты тензора напряжений для упругого материала вид:
$$\sigma_{11} = \lambda\Theta + 2\mu\frac{\partial u_1}{\partial  x}; \;\sigma_{22} = \lambda\Theta + 2\mu\frac{\partial u_2}{\partial y}; \; \sigma_{12} = \mu\bigg(\frac{\partial u_1}{\partial y} +\frac{\partial u_2}{\partial x}\bigg)\eqno(35)$$
$$\text{Где } \; \lambda,\mu - \text{ константы Ламе, и } \; \Theta = \frac{\partial u_1}{\partial x} + \frac{\partial u_2}{\partial y}.$$
Тогда уравнение равновесия $\sigma_{ij,j} = 0 $ можно записать в форме:
$$\frac{\partial \sigma_{11}}{\partial x} + \frac{\partial \sigma_{12}}{\partial y} = 0$$
$$\frac{\partial \sigma_{12}}{\partial x} + \frac{\partial \sigma_{22}}{\partial y} = 0$$
Заметим, что $\sigma_{11} + \sigma_{22}$ удовлетворяет уравнению: $$\nabla^2(\sigma_{11} + \sigma_{22}) = (\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2})(\sigma_{11} + \sigma_{22}) = 0 \eqno(58)$$ Введем функцию напряжений $\Phi(x,y):$
$$\sigma_{11} = \frac{\partial^2\Phi}{\partial y^2}; \; \sigma_{22} = \frac{\partial^2\Phi}{\partial x^2}; \;
\sigma_{12} = -\frac{\partial^2\Phi}{\partial x \partial y}\eqno(36)$$
Непосредственной подстановкой соотношений легко убедиться, что уравнения удовлетворяются тождественно.
\\Подставим последние выражения в (42) :
$$\nabla^2(\nabla^2\Phi) = \nabla^4\Phi = 0$$
То есть функция напряжений - бигармоническая функция.

\begin{center}
    \item \subsection{Аналитическое решение задачи Кирша.}
\end{center}
При отсутствии отверстия во всех точках пластины появились бы напряжения 
$$\sigma_{11} = P; \;\sigma_{22} = 0; \; \sigma_{12} = 0.$$ Этим напряжениям соответствует функция напряжений: $\Phi(x,y) = \frac{1}{2}Py^2$
Наличие малого отверстия меняет распределение напряжений в точках, расположенных вблизи отверстия и, по принципу Сен-Венана, почти не влияет на напряжения в точках, удаленных от него.\\
Местные напряжения быстро затухают, а следовательно, в точках, расстояние которых от отверстия велико по сравнению с $a$, изменения будут незначительными.\\
Проведя окружность достаточно большого радиуса $R$, можно считать, что напряжения в точках вне этой окружности не изменятся от наличия отверстия.
Учитывая, что основные изменения напряженного состояния происходят внутри кольца $a \leqslant x^2 + y^2 \leqslant R$ удобно рассмотреть задачу в полярных координатах, связанных с декартовыми соотношениями $x = rcos\varphi; \; y = rsin\varphi.$
Функция напряжений в полярных координатах:
$$\Phi(r,\varphi) = \frac{1}{2}Pr^2sin^2\varphi\eqno(37)$$
Это представление может быть преобразовано:
$$\Phi(r,\varphi) = \frac{1}{4}Pr^2(1+cos2\varphi)= \frac{1}{4}Pr^2 + \frac{1}{4}Pr^2cos2\varphi\eqno(38)$$
Тогда напряжения будут иметь вид:
$$\sigma_{rr} = \frac{1}{2}P + \frac{1}{2}Pcos2\varphi; \;\; \sigma_{\varphi\varphi} = \frac{1}{2}P + \frac{1}{2}Pcos2\varphi; \;\; \sigma_{r\varphi} = - \frac{1}{2}Psin2\varphi.\eqno(39)$$
На основании изложенного и применяя полуобратный метод Сен-Венана, для пластинки с отверстием, рассмотрим функцию напряжений следующего вида:
